<!DOCTYPE html>
<html>
  <head>
    
    <meta charset="utf-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <title>Projects</title>

    
    <link rel="stylesheet" href="/css/style.css">
  </head>
<body>

<div class="nav">
  <a href="/about">About</a>
  <a href="/projects">Projects</a>
  <a href="/">Home</a>
</div>


<h1>Projects</h1>







<h2 id="current">Current</h2>
<h3 id="verifying-in-hardware-implementations-of-cache-sidechannel-mitigation-techniques">Verifying in hardware implementations of cache side-channel mitigation techniques</h3>
<p>A great deal of cache side-channel mitigation techniques and designs have been implemented, but little work has gone into verifying that the implementations of these techniques would actually be secure. My research thus focuses on creating a set of experiments that will verify if the given security properties of a protected cache design are actually met in its real-world implementation. We will also develop experiments that will simulate hardware failures to see how these secure cache designs hold up when hardware inevitably fails.</p>
<h3 id="cache-compression-to-mitigate-cache-sidechannel-attacks">Cache compression to mitigate cache side-channel attacks</h3>
<p>Cache side channel attacks, which primarily target cloud services like AWS to steal our personal information, pose a great threat to our well-being. Mitigating cache side channel attacks is a uniquely difficult problem because these attacks exploit the fundamental mechanism of what makes a cache fast: storing frequently accessed data. Another difficulty is that previous cache protection measures are expensive, either slowing down a cache's performance or increasing a cache's energy consumption. My research looks into how we can apply <em>cache compression</em>, a technique originally designed to increase cache capacity, to obfuscate the cache accesses of vulnerable data while maintaining performance. If we can compress vulnerable cache lines with non-vulnerable cache lines and tie their cache accesses together, the additional cache capacity we gain from cache compression will offset the overhead of our technique.</p>
<h2 id="past">Past</h2>
<h3 id="evaluating-ssd-latency-predictors">Evaluating SSD latency predictors</h3>
<p><a href="https://newtraell.cs.uchicago.edu/files/tr_additional/TR-2019-17.pdf">Tech report</a></p>
<p><strong>Abstract.</strong> Cutting tail latencies at the millisecond level in internet services for good response times in data-parallel applications is possible by integrating MittOS, an OS/data center interface. Typically, MittOS analyzes white-box information of the internals of devices such as SSD's and decides if a given server can &ldquo;fast reject&rdquo; a service request. But
commercial SSD's have a black-box design, so MittOS researchers have developed machine learning models to determine if requests to commercial SSDâ€™s can be rejected or not. When run on CPUs, however, these models cannot predict in the time it takes an SSD to fully process a request, defeating MittOS's fast-rejecting abilities. We demonstrate that
ASICs such as the Efficient Inference Engine (EIE) accelerate the prediction times of these MittOS models well within the time it takes an SSD to complete a request at minimal cost, cutting SSD tail latencies. EIE achieves 2.01 us inference latency while incurring minimal area costs (20.4 mm^2) and power costs (0.29 W). We show that integrating machine
learning into the critical path of operating systems becomes cost-efficient and within reason.</p>


</body>

